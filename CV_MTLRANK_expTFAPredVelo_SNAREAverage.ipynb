{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Define and compile NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model(num_tf, num_feature_type):\n",
    "    inputs = [Input(shape = (num_feature_type,)) for i in range(num_tf)]\n",
    "    concat_layer = [Dense(1, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l1(0.01))(inp) for inp in inputs]\n",
    "    concat_layer = concatenate(concat_layer)\n",
    "    out = Dense(64, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l1(0.01))(concat_layer)\n",
    "    out = Dense(32, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l1(0.01))(out)\n",
    "    out = Dense(16, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l1(0.01))(out)\n",
    "    out = Dense(1, activation = \"linear\")(out)\n",
    "\n",
    "    # Skip model compilation step\n",
    "    model = Model(inputs = inputs, outputs = out)\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Load expression, TF activitiy scores, and ensembl ID mapping file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, read RPKM values and velocity values and then scale them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rpkm = dict()\n",
    "with h5py.File('data/processed/rpkm/rpkm_snare.hdf5', 'r') as f:\n",
    "    for tissue in f.keys():\n",
    "        \n",
    "        # scale the rpkm by log10\n",
    "        rpkm[tissue] = pd.DataFrame(np.log10(np.array(f[tissue]['exp'])+1),\n",
    "                                        index = np.array(f[tissue]['barcode']).astype(str),\n",
    "                                        columns = np.array(f[tissue]['ensembl']).astype(str))\n",
    "\n",
    "velo = dict()\n",
    "with h5py.File('data/processed/velo/velo_snare.hdf5', 'r') as f:\n",
    "    for tissue in f.keys():\n",
    "        velo[tissue] = pd.DataFrame(np.array(f[tissue]['velo']),\n",
    "                                        index = np.array(f[tissue]['barcode']).astype(str),\n",
    "                                        columns = np.array(f[tissue]['ensembl']).astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only TF as columns for rpkm matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep TFs as columns for rpkm mat.\n",
    "# Keep common genes between tf_gene mat and velocity mat.\n",
    "\n",
    "tf_list = pd.read_csv(\"data/raw/tf/tf_list.csv\", index_col = 0)\n",
    "for tissue in rpkm.keys():\n",
    "    \n",
    "    use_tfs = rpkm[tissue].columns.intersection(tf_list[\"Ensembl ID\"])\n",
    "    rpkm[tissue] = rpkm[tissue].loc[:,use_tfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Select genes for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select 500 genes in each tissue for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kidney_Left has 500 selected genes.\n",
      "Kidney_Right has 500 selected genes.\n",
      "Lung_Right has 500 selected genes.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "num_genes = 500\n",
    "num_samples = 4000\n",
    "\n",
    "selected_genes = dict()\n",
    "\n",
    "for tissue in rpkm.keys():\n",
    "        \n",
    "    # Select genes that have > 4000 samples\n",
    "    sample_counts = (~velo[tissue].isna()).sum(axis = 0)\n",
    "    genes = sample_counts.index[sample_counts > num_samples].values\n",
    "    np.random.seed(1000)\n",
    "    selected_genes[tissue] = np.random.choice(genes, num_genes, replace = False)\n",
    "    print(\"{} has {} selected genes.\".format(tissue, str(selected_genes[tissue].shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Cluster and order genes\n",
    "1. Cluster the selected velocity genes by velocity values (balanced kmeans clustering)\n",
    "2. In each cluster, order the genes by correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from k_means_constrained import KMeansConstrained\n",
    "cluster_labels = dict()\n",
    "\n",
    "for tissue in rpkm.keys():\n",
    "    \n",
    "    # Get velocity matrix rows are cells, cols are genes.\n",
    "    mat = velo[tissue].loc[:,selected_genes[tissue]].values\n",
    "    \n",
    "    # Impute the missing velocity values by mean velocity from all available cells.\n",
    "    na_mask = np.isnan(mat).astype(int)\n",
    "    mean_mat = np.nanmean(mat, axis = 0).reshape(1,-1) * na_mask\n",
    "    mat = np.nan_to_num(mat,nan = 0)\n",
    "    mat = mat + mean_mat\n",
    "    mat = pd.DataFrame(\n",
    "                        mat,\n",
    "                        columns = velo[tissue].loc[:,selected_genes[tissue]].columns\n",
    "                    )\n",
    "    \n",
    "    # Get cluster label for each gene.\n",
    "    cluster_labels[tissue] = KMeansConstrained(n_clusters=20, size_min=int(mat.shape[1]/20), size_max=np.ceil(mat.shape[1]/20)).fit_predict(mat.values.T)\n",
    "    cluster_labels[tissue] = pd.DataFrame(cluster_labels[tissue], index = mat.columns, columns = [\"cluster\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Define trace norm gradient (for loss function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define nuclear norm, (sub)gradient of nuclear norm, and tensor trace norm, and tensor unfold function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define nuclear norm operation\n",
    "@tf.custom_gradient\n",
    "def nuclear_norm(x):\n",
    "    sigma = tf.linalg.svd(x, full_matrices=False, compute_uv=False)\n",
    "    norm = tf.reduce_sum(sigma)\n",
    "    \n",
    "    # Grandient function\n",
    "    def nuclear_norm_grad(dy):\n",
    "        _, U, V = tf.linalg.svd(x, full_matrices=False, compute_uv=True)\n",
    "        grad = tf.matmul(U, tf.transpose(V))\n",
    "        return dy * grad\n",
    "    \n",
    "    return norm, nuclear_norm_grad\n",
    "\n",
    "def TensorUnfold(A, k):\n",
    "    tmp_arr = np.arange(A.shape.ndims)\n",
    "    A = tf.transpose(A, [tmp_arr[k]] + np.delete(tmp_arr, k).tolist())\n",
    "    A = tf.reshape(A, [A.shape[0], np.prod(A.shape[1:])])\n",
    "    return A\n",
    "\n",
    "def trace_norm(X):\n",
    "    return nuclear_norm(TensorUnfold(X, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Train and test MTLRANK models\n",
    "This step may take a long time to complte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import defaultdict\n",
    "from tensorflow import keras\n",
    "from joblib import Parallel,delayed\n",
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "n_rep = 3\n",
    "n_epoch = 3\n",
    "batch_size = 256\n",
    "test_ratio = 0.1\n",
    "num_cluster = 20\n",
    "\n",
    "# MSE function and adam optimizer\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "\n",
    "def train_test(tissue, cluster, rep, n_epochs):\n",
    "    \n",
    "    # Get velocity genes for current cluster\n",
    "    cluster_genes = cluster_labels[tissue].loc[cluster_labels[tissue].iloc[:,0] == cluster,].index.values\n",
    "    '''\n",
    "    # For each gene 1) Get train and test data.\n",
    "                    2) Standardize train and test data\n",
    "    '''\n",
    "    X_train = defaultdict(list)\n",
    "    X_test = defaultdict(list)\n",
    "    y_train = defaultdict(list)\n",
    "    y_test = defaultdict(list)\n",
    "    \n",
    "    print(\"Preparing features ... {}\".format(datetime.datetime.now().ctime()))\n",
    "    remain = dict() # Use this to keep track of which samples have been sampled for each gene.\n",
    "    for gene in cluster_genes:\n",
    "\n",
    "        # Reformat data to multiple inputs\n",
    "        y_train[gene] = velo[tissue][gene].values[train_idx[rep]]\n",
    "        y_test[gene] = velo[tissue][gene].values[test_idx[rep]]\n",
    "        \n",
    "        # Barcodes for the cells selected \n",
    "        use_cells_train = velo[tissue][gene].index.values[train_idx[rep]]\n",
    "        use_cells_test = velo[tissue][gene].index.values[test_idx[rep]]\n",
    "        \n",
    "        # Remove NA entries\n",
    "        select_train = ~np.isnan(y_train[gene])\n",
    "        select_test = ~np.isnan(y_test[gene])\n",
    "\n",
    "        y_train[gene] = y_train[gene][select_train]\n",
    "        y_test[gene] = y_test[gene][select_test]\n",
    "        \n",
    "        use_cells_train = use_cells_train[select_train]\n",
    "        use_cells_test = use_cells_test[select_test]\n",
    "        \n",
    "        # Standardize y\n",
    "        y_train[gene] = (y_train[gene] - np.mean(y_train[gene]))/np.std(y_train[gene])\n",
    "        y_test[gene] = (y_test[gene] - np.mean(y_test[gene]))/np.std(y_test[gene])\n",
    "\n",
    "        # Generate train/test inputs for current gene\n",
    "        for TF in rpkm[tissue].columns:\n",
    "\n",
    "            X_train[gene].append(np.zeros((train_idx[rep].shape[0], 5)))\n",
    "            X_test[gene].append(np.zeros((test_idx[rep].shape[0], 5)))\n",
    "\n",
    "            # Expression rpkms\n",
    "            X_train[gene][-1][:,0] = rpkm[tissue][TF].values[train_idx[rep],]\n",
    "            X_test[gene][-1][:,0] = rpkm[tissue][TF].values[test_idx[rep],]\n",
    "            \n",
    "            # Remove the NA entries\n",
    "            X_train[gene][-1] = X_train[gene][-1][select_train]\n",
    "            X_test[gene][-1] = X_test[gene][-1][select_test]\n",
    "            \n",
    "            # Use TF activities if they exist (tf from rpkm matrix), otherwise specify them as zero.\n",
    "            path = \"data/tf_activity_snare/{}/{}/{}/\".format(tissue,TF,gene)\n",
    "            if os.path.isdir(path):\n",
    "                # TF mean signals train\n",
    "                tf_weights = pd.DataFrame(np.zeros(use_cells_train.shape[0]), columns = [\"weight\"], index = use_cells_train)\n",
    "                nonzero_weights = pd.read_csv(path + \"mean.csv\",index_col = 1)\n",
    "                comm_cells = np.intersect1d(tf_weights.index, nonzero_weights.index)\n",
    "                #tf_weights.loc[comm_cells,\"weight\"] = nonzero_weights.loc[comm_cells,][\"weight\"].values\n",
    "                tf_weights.loc[:,\"weight\"] = nonzero_weights.loc[:,\"weight\"].values.mean()\n",
    "                X_train[gene][-1][:,1] = tf_weights[\"weight\"].values \n",
    "\n",
    "                # TF mean signals test\n",
    "                tf_weights = pd.DataFrame(np.zeros(use_cells_test.shape[0]), columns = [\"weight\"], index = use_cells_test)\n",
    "                nonzero_weights = pd.read_csv(path + \"mean.csv\",index_col = 1)\n",
    "                comm_cells = np.intersect1d(tf_weights.index, nonzero_weights.index)\n",
    "                #tf_weights.loc[comm_cells,\"weight\"] = nonzero_weights.loc[comm_cells,][\"weight\"].values\n",
    "                tf_weights.loc[:,\"weight\"] = nonzero_weights.loc[:,\"weight\"].values\n",
    "                X_test[gene][-1][:,1] = tf_weights[\"weight\"].values\n",
    "                \n",
    "                # TF sum signals train\n",
    "                tf_weights = pd.DataFrame(np.zeros(use_cells_train.shape[0]), columns = [\"weight\"], index = use_cells_train)\n",
    "                nonzero_weights = pd.read_csv(path + \"sum.csv\",index_col = 1)\n",
    "                comm_cells = np.intersect1d(tf_weights.index, nonzero_weights.index)\n",
    "                #tf_weights.loc[comm_cells,\"weight\"] = np.log2(nonzero_weights.loc[comm_cells,][\"weight\"].values + 1) # log2 scailing used\n",
    "                tf_weights.loc[:,\"weight\"] = np.log2(nonzero_weights.loc[:,\"weight\"].values + 1) # log2 scaled\n",
    "                X_train[gene][-1][:,2] = tf_weights[\"weight\"].values\n",
    "                \n",
    "                # TF sum signals test\n",
    "                tf_weights = pd.DataFrame(np.zeros(use_cells_test.shape[0]), columns = [\"weight\"], index = use_cells_test)\n",
    "                nonzero_weights = pd.read_csv(path + \"sum.csv\",index_col = 1)\n",
    "                comm_cells = np.intersect1d(tf_weights.index, nonzero_weights.index)\n",
    "                #tf_weights.loc[comm_cells,\"weight\"] = np.log2(nonzero_weights.loc[comm_cells,][\"weight\"].values + 1) # log2 scailing used\n",
    "                tf_weights.loc[:,\"weight\"] = np.log2(nonzero_weights.loc[:,\"weight\"].values + 1) # log2 scaled\n",
    "                X_test[gene][-1][:,2] = tf_weights[\"weight\"].values\n",
    "\n",
    "            # The product of expressions and TF mean signals\n",
    "            X_train[gene][-1][:,3] = X_train[gene][-1][:,0] * X_train[gene][-1][:,1]\n",
    "            X_test[gene][-1][:,3] = X_test[gene][-1][:,0] * X_test[gene][-1][:,1]\n",
    "\n",
    "            # The product of expressions and TF sum signals\n",
    "            X_train[gene][-1][:,4] = X_train[gene][-1][:,0] * X_train[gene][-1][:,2]\n",
    "            X_test[gene][-1][:,4] = X_test[gene][-1][:,0] * X_test[gene][-1][:,2]\n",
    "\n",
    "            # Standardize the rpkms and the prod features\n",
    "            if np.std(X_train[gene][-1][:,0]) != 0:\n",
    "                X_train[gene][-1][:,0] = (X_train[gene][-1][:,0] - np.mean(X_train[gene][-1][:,0]))/np.std(X_train[gene][-1][:,0])\n",
    "\n",
    "            if np.std(X_train[gene][-1][:,3]) != 0:\n",
    "                X_train[gene][-1][:,3] = (X_train[gene][-1][:,3] - np.mean(X_train[gene][-1][:,3]))/np.std(X_train[gene][-1][:,3])\n",
    "\n",
    "            if np.std(X_train[gene][-1][:,4]) != 0:\n",
    "                X_train[gene][-1][:,4] = (X_train[gene][-1][:,4] - np.mean(X_train[gene][-1][:,4]))/np.std(X_train[gene][-1][:,4])\n",
    "\n",
    "            if np.std(X_test[gene][-1][:,0]) != 0:\n",
    "                X_test[gene][-1][:,0] = (X_test[gene][-1][:,0] - np.mean(X_test[gene][-1][:,0]))/np.std(X_test[gene][-1][:,0])\n",
    "\n",
    "            if np.std(X_test[gene][-1][:,3]) != 0:\n",
    "                X_test[gene][-1][:,3] = (X_test[gene][-1][:,3] - np.mean(X_test[gene][-1][:,3]))/np.std(X_test[gene][-1][:,3])\n",
    "\n",
    "            if np.std(X_test[gene][-1][:,4]) != 0:\n",
    "                X_test[gene][-1][:,4] = (X_test[gene][-1][:,4] - np.mean(X_test[gene][-1][:,4]))/np.std(X_test[gene][-1][:,4])\n",
    "                \n",
    "    '''\n",
    "    Train by trace norm loss.\n",
    "    '''\n",
    "    print(\"Setting up models...{}\".format(datetime.datetime.now().ctime()))\n",
    "    # Build models without compilation. Each model corresponds to one gene in the cluster.\n",
    "    # Compile model and keep the initial weights\n",
    "    num_tf = rpkm[tissue].shape[1]\n",
    "    models = [build_model(num_tf = num_tf, num_feature_type = 5) for i in range(cluster_genes.shape[0])]\n",
    "    \n",
    "    # Get test batches\n",
    "    print(\"Running training...{}\".format(datetime.datetime.now().ctime()))\n",
    "    x_test_batches = []\n",
    "    y_test_batches = []\n",
    "    for k,gene in enumerate(cluster_genes):\n",
    "        x_test_batches.append(X_test[gene])\n",
    "        y_test_batches.append(y_test[gene])\n",
    "    \n",
    "    # One step of training\n",
    "    def train_step(x_batches, y_batches):\n",
    "        \n",
    "        # Record how loss value is computed for performing automatic differential later.\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # Run one round of forward pass for all models\n",
    "            y_pred = [models[k](x, training = True) for k,x in enumerate(x_batches)]\n",
    "            \n",
    "            # Compute MSE loss\n",
    "            MSE = [tf.reduce_mean(mse(y, y_pred[k])) for k,y in enumerate(y_batches)]\n",
    "            \n",
    "            # Get the weights of the last three layers. These layers are shared and their tracenorm loss\n",
    "            # will be calculated.\n",
    "            sharable_weights = []\n",
    "            \n",
    "            for layer in [num_tf*2, num_tf*2+2]:\n",
    "                sharable_weights.append(tf.stack([model.trainable_weights[layer] for model in models], axis = -1))    \n",
    "            \n",
    "            # Compute tracenorm\n",
    "            tracenorm = [trace_norm(sharable_weights[k]) for k in range(len(sharable_weights))]\n",
    "            \n",
    "            # Compute final loss. loss = MSE + lambda*tracenorm. (lambda = 0.01 for convenience)\n",
    "            loss = tf.reduce_mean(MSE) + tf.math.multiply(0.01, tf.reduce_mean(tracenorm))\n",
    "            \n",
    "       '''\n",
    "        Here gradients are calculated \n",
    "        'unconnected_gradients=tf.UnconnectedGradients.ZERO' ensures that\n",
    "        gradients for other task-specific layer other than the current one\n",
    "        are zeros.\n",
    "        '''\n",
    "        grads_all = tape.gradient(loss,\n",
    "                              [model.trainable_weights for model in models],\n",
    "                              unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
    "        \n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        for grad,model in zip(grads_all, models):\n",
    "            optimizer.apply_gradients(zip(grad, model.trainable_weights))\n",
    "       \n",
    "        return(loss, tf.reduce_mean(MSE))\n",
    "        \n",
    "    def test_step(models, x_batches, y_batches):\n",
    "        r2s = []\n",
    "        for x,y,model in zip(x_batches,y_batches,models):\n",
    "            y_pred = model.predict(x)\n",
    "            r2s.append(r2_score(y,y_pred))\n",
    "        return(r2s)\n",
    "        \n",
    "    # Train MTL models\n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        # Generate sampling indices for each batch\n",
    "        # Number of batches are dependent on the gene with the most training examples (N).\n",
    "        # For other genes, examples are resampled if they have smaller sample sizes.\n",
    "        tmp = [(gene,sample.shape[0]) for gene,sample in y_train.items()]\n",
    "        tmp = sorted(tmp, key = lambda x: x[1], reverse = True)\n",
    "        max_size = tmp[0][1]\n",
    "        \n",
    "        # Get sample indices for each training step\n",
    "        idx_iter = dict()\n",
    "        for gene,size in tmp:\n",
    "            idx = np.arange(size)\n",
    "            np.random.shuffle(idx)\n",
    "            idx = np.resize(idx, max_size)\n",
    "            idx_iter[gene] = np.split(idx, np.arange(batch_size,idx.shape[0],batch_size))\n",
    "        n_iter = len(idx_iter[gene])\n",
    "        \n",
    "        # Training loop\n",
    "        for j in range(n_iter):\n",
    "            begin = time()\n",
    "            \n",
    "            # Get training data batches for each training step (one batch per gene).\n",
    "            x_train_batches = []\n",
    "            y_train_batches = []\n",
    "            for k,gene in enumerate(cluster_genes):\n",
    "                x_train_batches.append([tf.convert_to_tensor(feature[idx_iter[gene][j],], dtype = tf.float64) for feature in X_train[gene]])\n",
    "                y_train_batches.append(tf.convert_to_tensor(y_train[gene][idx_iter[gene][j]], dtype = tf.float64)) \n",
    "                \n",
    "            # Run one step of training (Compute loss and perform one step of gradient descent), make sure all input arguments are tf.Tensor or list of tf.Tensor\n",
    "            loss, MSE = train_step(x_train_batches, y_train_batches)\n",
    "            end = time()\n",
    "            spent = np.round(end - begin, 1)\n",
    "            \n",
    "            print(\"Epoch {}/{}, Iter {}/{}, loss value: {}, MSE: {}, {}s used\".format(str(i+1),\n",
    "                                                                                str(n_epochs),\n",
    "                                                                                str(j+1),\n",
    "                                                                                str(n_iter),                                                                                          \n",
    "                                                                                str(loss.numpy()),\n",
    "                                                                                str(MSE.numpy()),\n",
    "                                                                                str(spent)))\n",
    "    \n",
    "    # Training finished, test models\n",
    "    r2s = test_step(models, x_test_batches, y_test_batches)\n",
    "\n",
    "    # Save output to file\n",
    "    cur_res = \"{},{},{},{}\\n\".format(tissue,gene,str(np.mean(r2s)),str(rep))\n",
    "    with open(\"/results/r2/MTLRANK_expTFAPredVelo_r2_SNAREAverage.csv\",\"a\") as f:\n",
    "        for r2,gene in zip(r2s,cluster_genes):\n",
    "            f.writelines(\"{},{},{},{}\\n\".format(tissue,gene,str(rep),str(r2)))\n",
    "\n",
    "    return(None)\n",
    "\n",
    "for tissue in list(rpkm.keys()):\n",
    "    \n",
    "    # Generate train and test sample indices for different replicates\n",
    "    idx = np.arange(rpkm[tissue].shape[0])\n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "    \n",
    "    # Generate train and test cell indices for different replicates\n",
    "    for rep in range(n_rep):\n",
    "        idx1, idx2 = train_test_split(idx, test_size = test_ratio, shuffle = True)\n",
    "        train_idx.append(idx1)\n",
    "        test_idx.append(idx2)\n",
    "    \n",
    "    # Run this number of replicates\n",
    "    for rep in range(n_rep):    \n",
    "    #Train and test genes in current cluster\n",
    "        res = Parallel(n_jobs = 2)(delayed(train_test)(tissue, cluster, rep, n_epochs = 3)\n",
    "                                    for cluster in range(num_cluster))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
