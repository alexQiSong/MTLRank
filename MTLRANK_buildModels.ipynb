{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TF list and ensembl to symbol mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tf_list = pd.read_csv(\"data/raw/tf/tf_list.csv\", index_col = 0)    \n",
    "ensembl_to_symbol = pd.read_csv(\"data/raw/id_mapping/ensembl_to_symbol.csv\",index_col = 0)\n",
    "ensembl_to_symbol = ensembl_to_symbol.loc[~ensembl_to_symbol[\"ensembl_id\"].duplicated(),:]\n",
    "ensembl_to_symbol.index = ensembl_to_symbol[\"ensembl_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load expression data, and TF activitiy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trains models for one tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tissue = 'Lung_Right'\n",
    "\n",
    "# First we get indices of TFs\n",
    "with h5py.File('data/processed/rpkm/rpkm.hdf5', 'r') as f:\n",
    "    col_names = np.array(f[tissue]['ensembl']).astype(str)\n",
    "tf_idx = np.where(np.isin(col_names, tf_list[\"Ensembl ID\"].values))[0]\n",
    "\n",
    "# Get expressions only for TF columns. RPKM values are log10 transformed.\n",
    "with h5py.File('data/processed/rpkm/rpkm.hdf5', 'r') as f:\n",
    "    rpkm = pd.DataFrame(np.log10(np.array(f[tissue]['exp'][:,tf_idx])+1),\n",
    "                                index = np.array(f[tissue]['barcode']).astype(str),\n",
    "                                columns = np.array(f[tissue]['ensembl'][tf_idx]).astype(str))\n",
    "\n",
    "with h5py.File('data/processed/velo/velo.hdf5', 'r') as f:\n",
    "    velo = pd.DataFrame(np.array(f[tissue]['velo']),\n",
    "                                index = np.array(f[tissue]['barcode']).astype(str),\n",
    "                                columns = np.array(f[tissue]['ensembl']).astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read TF activity score matrices. Missing values are treated as zeros.  \n",
    "This trains models for one tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder = \"data/processed/tf_activity/\"\n",
    "\n",
    "tf_gene_mat_mean = pd.read_csv(folder + \"mean_tf/\" + tissue + \"_tfGeneMat.csv\", index_col = 0).transpose()\n",
    "tf_gene_mat_sum = pd.read_csv(folder + \"sum_tf/\" + tissue + \"_tfGeneMat.csv\", index_col = 0).transpose()\n",
    "tf_gene_mat_mean.fillna(0,inplace = True)\n",
    "tf_gene_mat_sum.fillna(0,inplace = True)\n",
    "\n",
    "# scale the value by log2\n",
    "tf_gene_mat_mean = np.log2(tf_gene_mat_mean+1)\n",
    "tf_gene_mat_sum = np.log2(tf_gene_mat_sum+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat the data matrices to keep the consistent TFs and genes.\n",
    "Make rpkm matrix and tf activity matrix have the same tfs  \n",
    "Make velo matrix and tf activity matrix have the same genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_mat_sum = dict()\n",
    "act_mat_mean = dict()\n",
    "    \n",
    "# Find common TF \n",
    "tfmat_cols = pd.DataFrame(tf_gene_mat_mean.columns.values,columns = ['tfmat_cols'])\n",
    "comm_tfs = ensembl_to_symbol.merge(how = 'left',\n",
    "                                    left_on = 'gene_symbol',\n",
    "                                    right_on = \"tfmat_cols\",\n",
    "                                    right = tfmat_cols)\n",
    "comm_tfs = comm_tfs.loc[~comm_tfs['tfmat_cols'].isna(),]\n",
    "comm_tfs = comm_tfs.loc[comm_tfs[\"ensembl_id\"].isin(rpkm.columns),]\n",
    "\n",
    "# Find common (velocity) genes\n",
    "tfmat_rows = pd.DataFrame(tf_gene_mat_mean.index.values,columns = ['tfmat_rows'])\n",
    "comm_genes = ensembl_to_symbol.merge(how = 'left',\n",
    "                                        left_on = 'gene_symbol',\n",
    "                                        right_on = \"tfmat_rows\",\n",
    "                                        right = tfmat_rows)\n",
    "comm_genes = comm_genes.loc[~comm_genes['tfmat_rows'].isna(),]\n",
    "comm_genes = comm_genes.loc[comm_genes[\"ensembl_id\"].isin(velo.columns),]\n",
    "\n",
    "'''\n",
    "Reorder rpkm mat, velo mat, and cluster label vectors. The TF-Gene matrix follow the same tf_order and gene_order. In this order, the columns in upper left\n",
    "corner are the tfs commonly found in rpkm matrix and tf activity matrix and the rows represent the genes commonly found in velo matrix\n",
    "and tf activity matrix.\n",
    "          TF\n",
    "       _______ __\n",
    "      | common|  |\n",
    "Genes |_______|  |\n",
    "      |__________|\n",
    "'''\n",
    "tf_order = pd.concat([comm_tfs['ensembl_id'], pd.Series(np.setdiff1d(rpkm.columns, comm_tfs['ensembl_id']))])\n",
    "gene_order = pd.concat([comm_genes['ensembl_id'], pd.Series(np.setdiff1d(velo.columns, comm_genes['ensembl_id']))])\n",
    "\n",
    "rpkm = rpkm.loc[:,tf_order]\n",
    "velo = velo.loc[:,gene_order]\n",
    "\n",
    "act_mat_sum = np.zeros((velo.shape[1], rpkm.shape[1]))\n",
    "act_mat_mean = np.zeros((velo.shape[1], rpkm.shape[1]))\n",
    "\n",
    "act_mat_sum[:comm_genes.shape[0],:][:,:comm_tfs.shape[0]] = tf_gene_mat_sum.loc[comm_genes['tfmat_rows'],comm_tfs['tfmat_cols']]\n",
    "act_mat_mean[:comm_genes.shape[0],:][:,:comm_tfs.shape[0]] = tf_gene_mat_mean.loc[comm_genes['tfmat_rows'],comm_tfs['tfmat_cols']]\n",
    "\n",
    "act_mat_sum = pd.DataFrame(act_mat_sum, index = gene_order, columns = tf_order)\n",
    "act_mat_mean = pd.DataFrame(act_mat_mean, index = gene_order, columns = tf_order)\n",
    "\n",
    "del tf_gene_mat_mean\n",
    "del tf_gene_mat_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select genes\n",
    "- For liver, we get genes with more than 1000 cells for which the velocity values are available.\n",
    "- For other tissues, we get genes with more than 4000 cells for which the velocity values are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lung_Right has 2380 selected genes.\n"
     ]
    }
   ],
   "source": [
    "# Get cell counts for which the velocity values are available\n",
    "cell_counts = (~velo.isna()).sum(axis = 0)\n",
    "if tissue == \"Liver\":\n",
    "    selected_genes = cell_counts.index[cell_counts > 1000].values\n",
    "else:\n",
    "    selected_genes = cell_counts.index[cell_counts > 5000].values\n",
    "print(\"{} has {} selected genes.\".format(tissue, str(selected_genes.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster and order genes with balanced k-means clustering (by velocity values)\n",
    "1. Cluster the selected velocity genes by velocity values (balanced kmeans clustering)\n",
    "2. In each cluster, order the genes by correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from k_means_constrained import KMeansConstrained\n",
    "cluster_labels = dict()\n",
    "    \n",
    "# Get velocity matrix (rows are cells, cols are genes)\n",
    "mat = velo.loc[:,selected_genes].values\n",
    "\n",
    "# For each gene, impute the missing velocity values by mean velocity from all available cells.\n",
    "na_mask = np.isnan(mat).astype(int)\n",
    "mean_mat = np.nanmean(mat, axis = 0).reshape(1,-1) * na_mask\n",
    "mat = np.nan_to_num(mat,nan = 0)\n",
    "mat = mat + mean_mat\n",
    "mat = pd.DataFrame(\n",
    "                    mat,\n",
    "                    columns = velo.loc[:,selected_genes].columns\n",
    "                )\n",
    "\n",
    "# cluster size and number of clusters\n",
    "c_size_min = 24\n",
    "c_size_max = 25\n",
    "n_clusters = int(np.ceil(selected_genes.shape[0]/c_size_max))\n",
    "\n",
    "# Get cluster label for each gene.\n",
    "cluster_labels = KMeansConstrained(n_clusters = n_clusters,\n",
    "                                    size_min = c_size_min,\n",
    "                                    size_max = c_size_max).fit_predict(mat.values.T)\n",
    "cluster_labels = pd.DataFrame(cluster_labels, index = mat.columns, columns = [\"cluster\"])\n",
    "sizes = np.unique(cluster_labels['cluster'], return_counts=True)[1]\n",
    "print(\"Cluster sizes for {}: {}\".format(tissue, np.unique(cluster_labels['cluster'], return_counts=True)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = 1\n",
    "cluster_labels.to_csv(\"results/velo_clusters/{}-{}.csv\".format(tissue,rep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load clustering results and names of already trained genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "rep = 1\n",
    "cluster_labels = pd.read_csv(\"results/velo_clusters/{}-{}.csv\".format(tissue,rep), index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize velo\n",
    "velo = (velo - velo.mean())/velo.std()\n",
    "\n",
    "# standardize rpkm train\n",
    "rpkm = (rpkm - rpkm.mean())/rpkm.std()\n",
    "\n",
    "# Fill na as zeros in rpkm matrices\n",
    "rpkm.fillna(value = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and get models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MTL model architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_mtl_model(num_tf, num_feature_type, num_genes):\n",
    "    models = []\n",
    "    for k in range(num_genes):\n",
    "        inputs = [Input(shape = (num_feature_type,)) for i in range(num_tf)]\n",
    "        concat_layer = [Dense(1, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l1(0.01))(inp) for inp in inputs]\n",
    "        concat_layer = concatenate(concat_layer)\n",
    "        out = Dense(64, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l1(0.01))(concat_layer)\n",
    "        out = Dense(32, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l1(0.01))(out)\n",
    "        out = Dense(16, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l1(0.01))(out)\n",
    "        out = Dense(1, activation = \"linear\")(out)\n",
    "\n",
    "        models.append(Model(inputs = inputs, outputs = out))   \n",
    "        \n",
    "    return(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trace norm and gradient of trace norm**  \n",
    "Define nuclear norm, (sub)gradient of nuclear norm, and tensor trace norm, and tensor unfold function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define nuclear norm operation\n",
    "@tf.custom_gradient\n",
    "def nuclear_norm(x):\n",
    "    sigma = tf.linalg.svd(x, full_matrices=False, compute_uv=False)\n",
    "    norm = tf.reduce_sum(sigma)\n",
    "    \n",
    "    # Grandient function\n",
    "    def nuclear_norm_grad(dy):\n",
    "        _, U, V = tf.linalg.svd(x, full_matrices=False, compute_uv=True)\n",
    "        grad = tf.matmul(U, tf.transpose(V))\n",
    "        return dy * grad\n",
    "    \n",
    "    return norm, nuclear_norm_grad\n",
    "\n",
    "def TensorUnfold(A, k):\n",
    "    tmp_arr = np.arange(A.shape.ndims)\n",
    "    A = tf.transpose(A, tf.convert_to_tensor([tmp_arr[k]] + np.delete(tmp_arr, k).tolist()))\n",
    "    A = tf.reshape(A, tf.convert_to_tensor([A.shape[0], np.prod(A.shape[1:])]))\n",
    "    return A\n",
    "\n",
    "def trace_norm(X):\n",
    "    return nuclear_norm(TensorUnfold(X, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from time import time\n",
    "from joblib import Parallel,delayed\n",
    "\n",
    "import datetime\n",
    "\n",
    "def train(tissue, cluster, n_epochs, batch_size, rep):\n",
    "   \n",
    "    # Get velocity genes for current cluster\n",
    "    cluster_genes = cluster_labels.loc[cluster_labels.iloc[:,0] == cluster,].index.values\n",
    "    num_genes = cluster_genes.shape[0]\n",
    "    num_tf = rpkm.shape[1]    \n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train = defaultdict(list)\n",
    "    y_train = defaultdict(list)\n",
    "\n",
    "    print(\"Preparing features ... {}\".format(datetime.datetime.now().ctime()))\n",
    "    for gene in cluster_genes:\n",
    "\n",
    "        # Split train and test data for each gene\n",
    "        rpkm_train = rpkm\n",
    "        velo_train = velo[gene]\n",
    "\n",
    "        # Remove NAs.\n",
    "        select = ~pd.isna(velo_train)\n",
    "        velo_train = velo_train.loc[select]\n",
    "        rpkm_train = rpkm_train.loc[select,:]\n",
    "\n",
    "        # Generate train/test inputs for current gene\n",
    "        for i,TF in enumerate(rpkm_train.columns):\n",
    "\n",
    "            X_train[gene].append(np.empty((rpkm_train.shape[0], num_feature_type)))\n",
    "\n",
    "            # For current tf, get expression rpkms\n",
    "            X_train[gene][-1][:,0] = rpkm_train[TF].values\n",
    "\n",
    "            # Get TF mean signals and TF sum signals\n",
    "            X_train[gene][-1][:,1] = act_mat_mean.loc[gene,TF]\n",
    "            X_train[gene][-1][:,2] = act_mat_sum.loc[gene,TF]\n",
    "\n",
    "        y_train[gene] = velo_train.values\n",
    "    '''\n",
    "    Train by trace norm loss.\n",
    "    '''\n",
    "    print(\"Setting up models...{}\".format(datetime.datetime.now().ctime()))\n",
    "    # Build MTL model\n",
    "    models = build_mtl_model(num_tf = num_tf,\n",
    "                             num_feature_type = num_feature_type,\n",
    "                             num_genes = num_genes\n",
    "                            )\n",
    "    \n",
    "    # MSE loss function and adam optimizer\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "    \n",
    "    # Input signatures for training step as a tf.function\n",
    "    x_signatures = tf.data.DatasetSpec(tf.TensorSpec(shape = (num_tf, None, num_feature_type), dtype = tf.float32))\n",
    "    y_signatures = tf.data.DatasetSpec(tf.TensorSpec(shape = (None,), dtype = tf.float32))\n",
    "    \n",
    "    # training step and MTL loss function defined here\n",
    "    @tf.function(input_signature = (x_signatures, y_signatures))\n",
    "    def train_step(x_batches, y_batches):\n",
    "\n",
    "        # Record how loss value is computed for performing automatic differentiation later.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run one round of forward pass for all models\n",
    "            y_pred = tf.TensorArray(tf.float32, size = num_genes)\n",
    "            for k,X in zip(tuple(range(num_genes)), x_batches):\n",
    "                y_pred = y_pred.write(k, models[k](tf.unstack(X), training = True))\n",
    "            \n",
    "            # Compute MSE loss\n",
    "            MSE = tf.TensorArray(tf.float32, size = num_genes)\n",
    "            for k,y in zip(tuple(range(num_genes)), y_batches):\n",
    "                MSE = MSE.write(k, tf.reduce_mean(mse(y, y_pred.read(k))))\n",
    "            MSE = MSE.gather(tf.range(num_genes))\n",
    "            \n",
    "            # Get the weights of the first and second FC layers. These layers are shared and their tracenorm loss will be calculated.\n",
    "            sharable_weights = tf.TensorArray(tf.float32, size = 2)\n",
    "\n",
    "            # Concat the first sharable FC layer from all models\n",
    "            stacked_layers1 = tf.TensorArray(tf.float32, num_genes)\n",
    "            for k,model in enumerate(models):\n",
    "                stacked_layers1 = stacked_layers1.write(k, model.trainable_weights[num_tf*2])\n",
    "            stacked_layers1 = stacked_layers1.gather(tf.range(num_genes))\n",
    "            \n",
    "            # Concat the second sharable FC layer from all models\n",
    "            stacked_layers2 = tf.TensorArray(tf.float32, num_genes)\n",
    "            for k,model in enumerate(models):\n",
    "                stacked_layers2 = stacked_layers2.write(k, model.trainable_weights[num_tf*2 + 2])\n",
    "            stacked_layers2 = stacked_layers2.gather(tf.range(num_genes))\n",
    "            \n",
    "            # Compute tracenorm from the two concatenated layer weight matrices\n",
    "            tracenorm = tf.TensorArray(tf.float32, 2)    \n",
    "            tracenorm = tracenorm.write(0,trace_norm(stacked_layers1))\n",
    "            tracenorm = tracenorm.write(1,trace_norm(stacked_layers2))\n",
    "            tracenorm = tracenorm.gather(tf.range(2))\n",
    "            \n",
    "            # Compute final loss. loss = MSE + lambda*tracenorm. (lambda = 0.01 for convenience)\n",
    "            loss = tf.reduce_mean(MSE) + tf.math.multiply(0.01, tf.reduce_mean(tracenorm))\n",
    "\n",
    "        '''\n",
    "        Here gradients are calculated \n",
    "        'unconnected_gradients=tf.UnconnectedGradients.ZERO' ensures that\n",
    "        gradients for other task-specific layer other than the current one\n",
    "        are zeros.\n",
    "        '''\n",
    "        grads_all = tape.gradient(loss,\n",
    "                              [model.trainable_weights for model in models],\n",
    "                              unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
    "        \n",
    "        # Run one step of gradient descent by updating the weights.\n",
    "        for grad,model in zip(grads_all, models):\n",
    "            optimizer.apply_gradients(zip(grad, model.trainable_weights))\n",
    "        \n",
    "        return(loss, tf.reduce_mean(MSE))\n",
    "    \n",
    "    # Training epochs\n",
    "    for i in range(n_epochs):\n",
    "\n",
    "        # Generate sampling indices for each batch\n",
    "        # Number of batches are dependent on the gene with the most training examples (N).\n",
    "        # For other genes, examples are resampled if they have smaller sample sizes.\n",
    "        tmp = [(gene,sample.shape[0]) for gene,sample in y_train.items()]\n",
    "        tmp = sorted(tmp, key = lambda x: x[1], reverse = True)\n",
    "        max_size = tmp[0][1]\n",
    "\n",
    "        # Get sample indices for each training step\n",
    "        idx_iter = dict()\n",
    "        for gene,size in tmp:\n",
    "            idx = np.arange(size)\n",
    "            np.random.shuffle(idx)\n",
    "            idx = np.resize(idx, max_size)\n",
    "            idx_iter[gene] = np.split(idx, np.arange(batch_size,idx.shape[0],batch_size))\n",
    "        n_iter = len(idx_iter[gene])\n",
    "\n",
    "        # Training loop\n",
    "        for j in range(n_iter):\n",
    "            begin = time()\n",
    "\n",
    "            # Get training data batches for each training step (one batch per gene).\n",
    "            x_train_batches = []\n",
    "            y_train_batches = []\n",
    "            for k,gene in enumerate(cluster_genes):\n",
    "                x_train_batches.append([tf.cast(feature[idx_iter[gene][j],], dtype = tf.float32) for feature in X_train[gene]])\n",
    "                y_train_batches.append(tf.cast(y_train[gene][idx_iter[gene][j]], dtype = tf.float32)) \n",
    "            \n",
    "            # Wrap the data using tf.data.Dataset\n",
    "            x_batches = tf.data.Dataset.from_tensor_slices(x_train_batches)\n",
    "            y_batches = tf.data.Dataset.from_tensor_slices(y_train_batches)\n",
    "            \n",
    "            # Run one step of training\n",
    "            loss, MSE = train_step(x_batches, y_batches)\n",
    "            end = time()\n",
    "            spent = np.round(end - begin, 1)\n",
    "\n",
    "            print(\"Epoch {}/{}, Iter {}/{}, loss value: {}, MSE: {}, {}s used\".format(str(i+1),\n",
    "                                                                                str(n_epochs),\n",
    "                                                                                str(j+1),\n",
    "                                                                                str(n_iter),                                                                                          \n",
    "                                                                                str(loss.numpy()),\n",
    "                                                                                str(MSE.numpy()),\n",
    "                                                                                str(spent)))\n",
    "    '''\n",
    "    Save models\n",
    "    '''\n",
    "    print(\"Saving models ... {}\".format(datetime.datetime.now().ctime()))\n",
    "    for k,gene in enumerate(cluster_genes):\n",
    "        models[k].save_weights(\"results/full_model/{}-{}-rep{}\".format(gene,tissue,rep))\n",
    "    return(None)\n",
    "\n",
    "n_epochs = 4\n",
    "batch_size = 256\n",
    "tissue = 'Lung_Right' # Specify tissue to be trained here\n",
    "num_clusters = cluster_labels['cluster'].max() + 1\n",
    "num_feature_type = 3\n",
    "\n",
    "# Train ensemble model for each cluster, run for five reps.\n",
    "res = Parallel(n_jobs = 16)(delayed(train)(tissue, cluster, n_epochs, batch_size, rep)\n",
    "                              for cluster,rep in list(zip(np.tile(np.arange(num_clusters),1), np.repeat(np.arange(5),num_clusters))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
