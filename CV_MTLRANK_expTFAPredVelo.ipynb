{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Define the basic NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model(num_tf, num_feature_type):\n",
    "    inputs = [Input(shape = (num_feature_type,)) for i in range(num_tf)]\n",
    "    concat_layer = [Dense(1, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l1(0.01))(inp) for inp in inputs]\n",
    "    concat_layer = concatenate(concat_layer)\n",
    "    out = Dense(64, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l1(0.01))(concat_layer)\n",
    "    out = Dense(32, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l1(0.01))(out)\n",
    "    out = Dense(16, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l1(0.01))(out)\n",
    "    out = Dense(1, activation = \"linear\")(out)\n",
    "\n",
    "    # Skip model compilation step\n",
    "    model = Model(inputs = inputs, outputs = out)\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Load expression, TF activitiy scores, and ensmebl to symbol mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, read RPKM values and velocity values and then scale them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rpkm = dict()\n",
    "with h5py.File('data/processed/rpkm/rpkm.hdf5', 'r') as f:\n",
    "    for tissue in f.keys():\n",
    "        \n",
    "        # scale the rpkm by log10\n",
    "        rpkm[tissue] = pd.DataFrame(np.log10(np.array(f[tissue]['exp'])+1),\n",
    "                                        index = np.array(f[tissue]['barcode']).astype(str),\n",
    "                                        columns = np.array(f[tissue]['ensembl']).astype(str))\n",
    "\n",
    "velo = dict()\n",
    "with h5py.File('data/processed/velo/velo.hdf5', 'r') as f:\n",
    "    for tissue in f.keys():\n",
    "        velo[tissue] = pd.DataFrame(np.array(f[tissue]['velo']),\n",
    "                                        index = np.array(f[tissue]['barcode']).astype(str),\n",
    "                                        columns = np.array(f[tissue]['ensembl']).astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read TF activity score matrices. Missing values are treated as zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder = \"data/tf_activity/\"\n",
    "\n",
    "tf_gene_mat_mean = dict()\n",
    "for filename in os.listdir(folder + \"mean_tf/\"):\n",
    "    tissue = \"_\".join(filename.split(\"_\")[:-1])\n",
    "    tf_gene_mat_mean[tissue] = pd.read_csv(\"{}/mean_tf/{}\".format(folder,filename),index_col = 0).transpose()\n",
    "    tf_gene_mat_mean[tissue].fillna(0,inplace = True)\n",
    "\n",
    "tf_gene_mat_sum = dict()\n",
    "for filename in os.listdir(folder + \"sum_tf/\"):\n",
    "    tissue = \"_\".join(filename.split(\"_\")[:-1])\n",
    "    tf_gene_mat_sum[tissue] = pd.read_csv(\"{}/sum_tf/{}\".format(folder,filename),index_col = 0).transpose()\n",
    "    tf_gene_mat_sum[tissue].fillna(0,inplace = True)\n",
    "    \n",
    "    # scale the value by log2\n",
    "    tf_gene_mat_sum[tissue] = np.log2(tf_gene_mat_sum[tissue]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read ensmebl to hgnc symbol mapping table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_to_symbol = pd.read_csv(\"data/raw/id_mapping/ensembl_to_symbol.csv\",index_col = 0)\n",
    "ensembl_to_symbol = ensembl_to_symbol.loc[~ensembl_to_symbol[\"ensembl_id\"].duplicated(),:]\n",
    "ensembl_to_symbol.index = ensembl_to_symbol[\"ensembl_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Select genes for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select 500 genes in each tissue for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "l1_res = pd.read_csv(\"results/r2/lasso_r2.csv\", index_col=0)\n",
    "\n",
    "num_genes = 500\n",
    "num_samples = 4000\n",
    "\n",
    "# Remove parenthesis and spaces in the tissue name\n",
    "l1_res[\"tissue\"] = l1_res['tissue'].str.replace('\\s+',\"_\")\n",
    "l1_res[\"tissue\"] = l1_res['tissue'].str.replace('\\(',\"\")\n",
    "l1_res[\"tissue\"] = l1_res['tissue'].str.replace('\\)',\"\")\n",
    "\n",
    "selected_genes = dict()\n",
    "\n",
    "for tissue,df in l1_res.groupby(\"tissue\"):\n",
    "    \n",
    "    if tissue in comm_tissues:\n",
    "        \n",
    "        # Select genes that have > 4000 samples\n",
    "        # For Liver, because there fewer samples, threshold is 1000 samples\n",
    "        sample_counts = (~velo[tissue].isna()).sum(axis = 0)\n",
    "        if tissue == \"Liver\":\n",
    "            genes = sample_counts.index[sample_counts > 1000].values\n",
    "        else:\n",
    "            genes = sample_counts.index[sample_counts > 4000].values\n",
    "        genes = np.intersect1d(rpkm[tissue].columns, genes)\n",
    "        df = df.loc[df[\"gene_ensembl\"].isin(genes),]\n",
    "\n",
    "        # Randomly select 500 genes in each tissue\n",
    "        np.random.seed(1000)\n",
    "        if num_genes < df.shape[0]:\n",
    "            idx = np.random.choice(np.arange(df.shape[0]), size=num_genes, replace=False)\n",
    "        else:\n",
    "            idx = np.arange(df.shape[0])\n",
    "        selected_genes[tissue] = df.iloc[idx,][\"gene_ensembl\"].values\n",
    "        print(\"{} has {} selected genes.\".format(tissue, str(selected_genes[tissue].shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Cluster and order genes  \n",
    "Cluster the selected velocity genes by velocity values using balanced kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from k_means_constrained import KMeansConstrained\n",
    "cluster_labels = dict()\n",
    "\n",
    "for tissue in velo.keys():\n",
    "    \n",
    "    # Get velocity matrix rows are cells, cols are genes.\n",
    "    mat = velo[tissue].loc[:,selected_genes[tissue]].values\n",
    "    \n",
    "    # Impute the missing velocity values by mean velocity from all available cells.\n",
    "    na_mask = np.isnan(mat).astype(int)\n",
    "    mean_mat = np.nanmean(mat, axis = 0).reshape(1,-1) * na_mask\n",
    "    mat = np.nan_to_num(mat,nan = 0)\n",
    "    mat = mat + mean_mat\n",
    "    mat = pd.DataFrame(\n",
    "                        mat,\n",
    "                        columns = velo[tissue].loc[:,selected_genes[tissue]].columns\n",
    "                    )\n",
    "    \n",
    "    # Get cluster label for each gene.\n",
    "    cluster_labels[tissue] = KMeansConstrained(n_clusters=20, size_min=int(mat.shape[1]/20), size_max=np.ceil(mat.shape[1]/20)).fit_predict(mat.values.T)\n",
    "    cluster_labels[tissue] = pd.DataFrame(cluster_labels[tissue], index = mat.columns, columns = [\"cluster\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the TFs as columns for rpkm mat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_list = pd.read_csv(\"data/raw/tf/tf_list.csv\", index_col = 0)\n",
    "for tissue in common_tissues:\n",
    "    \n",
    "    use_tfs = rpkm[tissue].columns.intersection(tf_list[\"Ensembl ID\"])\n",
    "    rpkm[tissue] = rpkm[tissue].loc[:,use_tfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Reformat the data matrices to keep the consistent TFs and genes across different data matrices\n",
    "Make rpkm matrix and tf activity matrix have the same tfs  \n",
    "Make velo matrix and tf activity matrix have the same genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_mat_sum = dict()\n",
    "act_mat_mean = dict()\n",
    "\n",
    "for tissue in tf_gene_mat_mean.keys():\n",
    "    \n",
    "    # Find common TF \n",
    "    tfmat_cols = pd.DataFrame(tf_gene_mat_mean[tissue].columns.values,columns = ['tfmat_cols'])\n",
    "    comm_tfs = ensembl_to_symbol.merge(how = 'left',\n",
    "                                        left_on = 'gene_symbol',\n",
    "                                        right_on = \"tfmat_cols\",\n",
    "                                        right = tfmat_cols)\n",
    "    comm_tfs = comm_tfs.loc[~comm_tfs['tfmat_cols'].isna(),]\n",
    "    comm_tfs = comm_tfs.loc[comm_tfs[\"ensembl_id\"].isin(rpkm[tissue].columns),]\n",
    "    \n",
    "    # Find common (velocity) genes\n",
    "    tfmat_rows = pd.DataFrame(tf_gene_mat_mean[tissue].index.values,columns = ['tfmat_rows'])\n",
    "    comm_genes = ensembl_to_symbol.merge(how = 'left',\n",
    "                                            left_on = 'gene_symbol',\n",
    "                                            right_on = \"tfmat_rows\",\n",
    "                                            right = tfmat_rows)\n",
    "    comm_genes = comm_genes.loc[~comm_genes['tfmat_rows'].isna(),]\n",
    "    comm_genes = comm_genes.loc[comm_genes[\"ensembl_id\"].isin(velo[tissue].columns),]\n",
    "    \n",
    "    '''\n",
    "    Reorder rpkm mat, velo mat, and cluster label vectors. The TF-Gene matrix follow the same tf_order and gene_order. In this order, the columns in upper left\n",
    "    corner are the tfs commonly found in rpkm matrix and tf activity matrix and the rows represent the genes commonly found in velo matrix\n",
    "    and tf activity matrix.\n",
    "              TF\n",
    "           _______ __\n",
    "          | common|  |\n",
    "    Genes |_______|  |\n",
    "          |__________|\n",
    "    '''\n",
    "    tf_order = pd.concat([comm_tfs['ensembl_id'], pd.Series(np.setdiff1d(rpkm[tissue].columns, comm_tfs['ensembl_id']))])\n",
    "    gene_order = pd.concat([comm_genes['ensembl_id'], pd.Series(np.setdiff1d(velo[tissue].columns, comm_genes['ensembl_id']))])\n",
    "    \n",
    "    rpkm[tissue] = rpkm[tissue].loc[:,tf_order]\n",
    "    velo[tissue] = velo[tissue].loc[:,gene_order]\n",
    "    \n",
    "    act_mat_sum[tissue] = np.zeros((velo[tissue].shape[1], rpkm[tissue].shape[1]))\n",
    "    act_mat_mean[tissue] = np.zeros((velo[tissue].shape[1], rpkm[tissue].shape[1]))\n",
    "    \n",
    "    act_mat_sum[tissue][:comm_genes.shape[0],:][:,:comm_tfs.shape[0]] = tf_gene_mat_sum[tissue].loc[comm_genes['tfmat_rows'],comm_tfs['tfmat_cols']]\n",
    "    act_mat_mean[tissue][:comm_genes.shape[0],:][:,:comm_tfs.shape[0]] = tf_gene_mat_mean[tissue].loc[comm_genes['tfmat_rows'],comm_tfs['tfmat_cols']]\n",
    "    \n",
    "    act_mat_sum[tissue] = pd.DataFrame(act_mat_sum[tissue], index = gene_order, columns = tf_order)\n",
    "    act_mat_mean[tissue] = pd.DataFrame(act_mat_mean[tissue], index = gene_order, columns = tf_order)\n",
    "\n",
    "del tf_gene_mat_mean\n",
    "del tf_gene_mat_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Define trace norm gradient (for loss function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define nuclear norm, (sub)gradient of nuclear norm, and tensor trace norm, and tensor unfold function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define nuclear norm operation\n",
    "@tf.custom_gradient\n",
    "def nuclear_norm(x):\n",
    "    sigma = tf.linalg.svd(x, full_matrices=False, compute_uv=False)\n",
    "    norm = tf.reduce_sum(sigma)\n",
    "    \n",
    "    # Grandient function\n",
    "    def nuclear_norm_grad(dy):\n",
    "        _, U, V = tf.linalg.svd(x, full_matrices=False, compute_uv=True)\n",
    "        grad = tf.matmul(U, tf.transpose(V))\n",
    "        return dy * grad\n",
    "    \n",
    "    return norm, nuclear_norm_grad\n",
    "\n",
    "def TensorUnfold(A, k):\n",
    "    tmp_arr = np.arange(A.shape.ndims)\n",
    "    A = tf.transpose(A, [tmp_arr[k]] + np.delete(tmp_arr, k).tolist())\n",
    "    A = tf.reshape(A, [A.shape[0], np.prod(A.shape[1:])])\n",
    "    return A\n",
    "\n",
    "def trace_norm(X):\n",
    "    return nuclear_norm(TensorUnfold(X, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Train and test MTLRANK models  \n",
    "This step may take a long time to complte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, psutil\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import defaultdict\n",
    "from tensorflow import keras\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "\n",
    "# MSE function and sgd optimizer\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "\n",
    "# Function for training and testing each gene\n",
    "def train_test(tissue, cluster, rep, n_epochs):\n",
    "    \n",
    "    # Get velocity genes for current cluster\n",
    "    cluster_genes = cluster_labels[tissue].loc[cluster_labels[tissue].iloc[:,0] == cluster,].index.values\n",
    "    '''\n",
    "    # For each gene 1) Get train and test data.\n",
    "                    2) Standardize train and test data\n",
    "    '''\n",
    "    X_train = defaultdict(list)\n",
    "    X_test = defaultdict(list)\n",
    "    y_train = defaultdict(list)\n",
    "    y_test = defaultdict(list)\n",
    "    \n",
    "    print(\"Preparing features ... {}\".format(datetime.datetime.now().ctime()))\n",
    "    remain = dict() # Use this to keep track of which cells have been used for each gene.\n",
    "    for gene in cluster_genes:\n",
    "        \n",
    "        # Split train and test data for each gene\n",
    "        rpkm_train = rpkm[tissue].iloc[train_idx[rep],]\n",
    "        rpkm_test = rpkm[tissue].iloc[test_idx[rep],]\n",
    "        velo_train = velo[tissue].iloc[train_idx[rep]][gene]\n",
    "        velo_test = velo[tissue].iloc[test_idx[rep]][gene]\n",
    "        \n",
    "        # Remove NAs.\n",
    "        select = ~pd.isna(velo_train)\n",
    "        velo_train = velo_train.loc[select]\n",
    "        rpkm_train = rpkm_train.loc[select,:]\n",
    "\n",
    "        select = ~pd.isna(velo_test)\n",
    "        velo_test = velo_test.loc[select]\n",
    "        rpkm_test = rpkm_test.loc[select,:]\n",
    "        \n",
    "        # Generate train/test inputs for current gene\n",
    "        for i,TF in enumerate(rpkm_train.columns):\n",
    "            \n",
    "            X_train[gene].append(np.empty((rpkm_train.shape[0], 5)))\n",
    "            X_test[gene].append(np.empty((rpkm_test.shape[0], 5)))\n",
    "            \n",
    "            # For current tf, get expression rpkms\n",
    "            X_train[gene][-1][:,0] = rpkm_train[TF].values\n",
    "            X_test[gene][-1][:,0] = rpkm_test[TF].values\n",
    "            \n",
    "            # Get TF mean signals and TF sum signals\n",
    "            X_train[gene][-1][:,1] = act_mat_mean[tissue].loc[gene,TF]\n",
    "            X_train[gene][-1][:,2] = act_mat_sum[tissue].loc[gene,TF]\n",
    "            X_test[gene][-1][:,1] = act_mat_mean[tissue].loc[gene,TF]\n",
    "            X_test[gene][-1][:,2] = act_mat_sum[tissue].loc[gene,TF]\n",
    "\n",
    "            # The product of expressions and TF mean signals\n",
    "            X_train[gene][-1][:,3] =  X_train[gene][-1][:,0] * X_train[gene][-1][:,1]\n",
    "            X_test[gene][-1][:,3] =  X_test[gene][-1][:,0] * X_test[gene][-1][:,1]\n",
    "\n",
    "            # The product of expressions and TF sum signals\n",
    "            X_train[gene][-1][:,4] = X_train[gene][-1][:,0] * X_train[gene][-1][:,2]\n",
    "            X_test[gene][-1][:,4] = X_test[gene][-1][:,0] * X_test[gene][-1][:,2]\n",
    "        \n",
    "            # Standardize the inputs\n",
    "            if(np.std(X_train[gene][-1][:,0]) != 0):\n",
    "                X_train[gene][-1][:,0] = (X_train[gene][-1][:,0] - np.mean(X_train[gene][-1][:,0]))/np.std(X_train[gene][-1][:,0])\n",
    "           \n",
    "            if(np.std(X_train[gene][-1][:,3]) != 0):\n",
    "                X_train[gene][-1][:,3] = (X_train[gene][-1][:,3] - np.mean(X_train[gene][-1][:,3]))/np.std(X_train[gene][-1][:,3])\n",
    "            \n",
    "            if(np.std(X_train[gene][-1][:,4]) != 0):\n",
    "                X_train[gene][-1][:,4] = (X_train[gene][-1][:,4] - np.mean(X_train[gene][-1][:,4]))/np.std(X_train[gene][-1][:,4])\n",
    "           \n",
    "            if(np.std(X_test[gene][-1][:,0]) != 0):\n",
    "                X_test[gene][-1][:,0] = (X_test[gene][-1][:,0] - np.mean(X_test[gene][-1][:,0]))/np.std(X_test[gene][-1][:,0])\n",
    "           \n",
    "            if(np.std(X_test[gene][-1][:,3]) != 0):\n",
    "                X_test[gene][-1][:,3] = (X_test[gene][-1][:,3] - np.mean(X_test[gene][-1][:,3]))/np.std(X_test[gene][-1][:,3])\n",
    "            \n",
    "            if(np.std(X_test[gene][-1][:,4]) != 0):\n",
    "                X_test[gene][-1][:,4] = (X_test[gene][-1][:,4] - np.mean(X_test[gene][-1][:,4]))/np.std(X_test[gene][-1][:,4])\n",
    "           \n",
    "        y_train[gene] = velo_train.values\n",
    "        y_test[gene] = velo_test.values\n",
    "        \n",
    "        y_train[gene] = (y_train[gene] - np.mean(y_train[gene]))/np.std(y_train[gene])\n",
    "        y_test[gene] = (y_test[gene] - np.mean(y_test[gene]))/np.std(y_test[gene])\n",
    "        \n",
    "    '''\n",
    "    Train by trace norm loss.\n",
    "    '''\n",
    "    print(\"Setting up models...{}\".format(datetime.datetime.now().ctime()))\n",
    "    \n",
    "    # Build models without compilation. Each model corresponds to one gene in the cluster.\n",
    "    num_tf = rpkm[tissue].shape[1]\n",
    "    models = [build_model(num_tf = num_tf, num_feature_type = 5) for i in range(cluster_genes.shape[0])]\n",
    "    \n",
    "    # Get test batches\n",
    "    print(\"Running training...{}\".format(datetime.datetime.now().ctime()))\n",
    "    x_test_batches = []\n",
    "    y_test_batches = []\n",
    "    for k,gene in enumerate(cluster_genes):\n",
    "        x_test_batches.append(X_test[gene])\n",
    "        y_test_batches.append(y_test[gene])\n",
    "    \n",
    "    # Training step for one batch of data (Each batch consists of a batch with batch_size cells from each gene)\n",
    "    # So the total number of cells in a batch = batch_size * number of genes\n",
    "    def train_step(x_batches, y_batches):\n",
    "        \n",
    "        # Record how loss value is computed for performing automatic differential later.\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # Run one round of forward pass for all models\n",
    "            y_pred = [models[k](x, training = True) for k,x in enumerate(x_batches)]\n",
    "            \n",
    "            # Compute MSE loss\n",
    "            MSE = [tf.reduce_mean(mse(y, y_pred[k])) for k,y in enumerate(y_batches)]\n",
    "            \n",
    "            # Get the weights of the last three layers. These layers are shared and their tracenorm loss\n",
    "            # will be calculated.\n",
    "            sharable_weights = []\n",
    "            \n",
    "            for layer in [num_tf*2, num_tf*2+2]:\n",
    "                sharable_weights.append(tf.stack([model.trainable_weights[layer] for model in models], axis = -1))    \n",
    "            \n",
    "            # Compute tracenorm\n",
    "            tracenorm = [trace_norm(sharable_weights[k]) for k in range(len(sharable_weights))]\n",
    "            \n",
    "            # Compute final loss. loss = MSE + lambda*tracenorm. (lambda = 0.01 for convenience)\n",
    "            loss = tf.reduce_mean(MSE) + tf.math.multiply(0.01, tf.reduce_mean(tracenorm))\n",
    "        \n",
    "        '''\n",
    "        Here gradients are calculated \n",
    "        'unconnected_gradients=tf.UnconnectedGradients.ZERO' ensures that\n",
    "        gradients for other task-specific layer other than the current one\n",
    "        are zeros.\n",
    "        '''\n",
    "        grads_all = tape.gradient(loss,\n",
    "                              [model.trainable_weights for model in models],\n",
    "                              unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
    "        \n",
    "        '''\n",
    "        Run one step of gradient descent by updating\n",
    "        the value of the variables to minimize the loss.\n",
    "        '''\n",
    "        for grad,model in zip(grads_all, models):\n",
    "            optimizer.apply_gradients(zip(grad, model.trainable_weights))\n",
    "       \n",
    "        return(loss, tf.reduce_mean(MSE))\n",
    "    \n",
    "    # Test step for testing one batch (Each batch consists of a batch with batch_size cells from each gene)\n",
    "    def test_step(models, x_batches, y_batches):\n",
    "        r2s = []\n",
    "        for x,y,model in zip(x_batches,y_batches,models):\n",
    "            y_pred = model.predict(x)\n",
    "            r2s.append(r2_score(y,y_pred))\n",
    "        return(r2s)\n",
    "    \n",
    "    # Main training loop\n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        # Generate sampling indices for each batch\n",
    "        # Number of batches are dependent on the gene with the most training examples (N).\n",
    "        # For other genes, examples are resampled if they have smaller sample sizes.\n",
    "        tmp = [(gene,sample.shape[0]) for gene,sample in y_train.items()]\n",
    "        tmp = sorted(tmp, key = lambda x: x[1], reverse = True)\n",
    "        max_size = tmp[0][1]\n",
    "        \n",
    "        # Get sample indices for each training step\n",
    "        idx_iter = dict()\n",
    "        for gene,size in tmp:\n",
    "            idx = np.arange(size)\n",
    "            np.random.shuffle(idx)\n",
    "            idx = np.resize(idx, max_size)\n",
    "            idx_iter[gene] = np.split(idx, np.arange(batch_size,idx.shape[0],batch_size))\n",
    "        n_iter = len(idx_iter[gene])\n",
    "        \n",
    "        # Training loop\n",
    "        for j in range(n_iter):\n",
    "            begin = time()\n",
    "            \n",
    "            # Get training data batches for each training step (one batch per gene).\n",
    "            x_train_batches = []\n",
    "            y_train_batches = []\n",
    "            for k,gene in enumerate(cluster_genes):\n",
    "                x_train_batches.append([tf.convert_to_tensor(feature[idx_iter[gene][j],], dtype = tf.float64) for feature in X_train[gene]])\n",
    "                y_train_batches.append(tf.convert_to_tensor(y_train[gene][idx_iter[gene][j]], dtype = tf.float64)) \n",
    "                \n",
    "            # Run one step of training (Compute loss and perform one step of gradient descent), make sure all input arguments are tf.Tensor or list of tf.Tensor\n",
    "            loss, MSE = train_step(x_train_batches, y_train_batches)\n",
    "            end = time()\n",
    "            spent = np.round(end - begin, 1)\n",
    "            \n",
    "            print(\"Epoch {}/{}, Iter {}/{}, loss value: {}, MSE: {}, {}s used\".format(str(i+1),\n",
    "                                                                                str(n_epochs),\n",
    "                                                                                str(j+1),\n",
    "                                                                                str(n_iter),                                                                                          \n",
    "                                                                                str(loss.numpy()),\n",
    "                                                                                str(MSE.numpy()),\n",
    "                                                                                str(spent)))\n",
    "            \n",
    "    r2s = test_step(models, x_test_batches, y_test_batches)\n",
    "    with open(\"results/r2/MTLRANK_expTFAPredVelo_r2.csv\",\"a\") as f:\n",
    "        for r2,gene in zip(r2s,cluster_genes):\n",
    "            f.writelines(\"{},{},{},{}\\n\".format(tissue, gene, str(rep), str(r2)))\n",
    "    \n",
    "    return(r2s)\n",
    "\n",
    "# Some hyperparameters \n",
    "n_rep = 3\n",
    "num_cluster = 20\n",
    "n_epochs = 10\n",
    "batch_size = 256\n",
    "test_ratio = 0.1\n",
    "r2_tissue = dict()\n",
    "batch_size = 256\n",
    "\n",
    "for tissue in velo.keys():\n",
    "    \n",
    "    # Generate train and test sample indices for different replicates\n",
    "    idx = np.arange(rpkm[tissue].shape[0])\n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "    \n",
    "    # Generate train and test cell indices for different replicates\n",
    "    for rep in range(n_rep):\n",
    "        idx1, idx2 = train_test_split(idx, test_size = test_ratio, shuffle = True)\n",
    "        train_idx.append(idx1)\n",
    "        test_idx.append(idx2)\n",
    "    \n",
    "    # Run this number of replicates\n",
    "    for rep in [2]:\n",
    "        \n",
    "        #Train and test genes in current cluster\n",
    "        res = Parallel(n_jobs = 4)(delayed(train_test)(tissue, cluster, rep, n_epochs = 3)\n",
    "                                    #for cluster in range(num_cluster))\n",
    "                                   for cluster in range(10,20,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
